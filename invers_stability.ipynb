{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    " !rm -rf runs #remove old runs from tensorboard (restart kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bIWdYFVVe6Ew"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wfKl33mNzrRj"
   },
   "outputs": [],
   "source": [
    "#network\n",
    "b = 1. #initial translation\n",
    "a = 1. #initial slope\n",
    "\n",
    "#validation set\n",
    "m = 1000 #size\n",
    "u, v = -50., 50. #domain [u, v]\n",
    "\n",
    "#loss and optimizer\n",
    "batchsize = 100\n",
    "lr = 1e-3\n",
    "iterations = 20000 #number of gradient descents\n",
    "criterion = nn.MSELoss() #loss function\n",
    "\n",
    "#regularization\n",
    "regularize = True #use gradient regularization\n",
    "reg_scale = 0.5 #scaling of the regularizer\n",
    "\n",
    "#tensorboard\n",
    "logging = False #turn of tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NNcFjXxpfxKi"
   },
   "outputs": [],
   "source": [
    "#network\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(1, 2)\n",
    "        self.fc1.weight.data=torch.tensor([[1.], [1.]])\n",
    "        self.fc1.bias.data=torch.tensor([0., -b])\n",
    "        self.fc2 = nn.Linear(2, 1)\n",
    "        self.fc2.weight.data=torch.tensor([[a, -a]])\n",
    "        self.fc2.bias.data=torch.tensor([0.])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "#tensorboard\n",
    "if logging:\n",
    "        from tensorboardX import SummaryWriter\n",
    "        writer = SummaryWriter()\n",
    "    \n",
    "#optimization\n",
    "def optimize(iterations, net):\n",
    "    \n",
    "    #validation set\n",
    "    x_val = torch.linspace(u, v, m).view(-1,1)\n",
    "    x_val.requires_grad = True\n",
    "    y_val = torch.zeros(m, 1)\n",
    "    \n",
    "    optimizer = optim.SGD(net.parameters(), lr=lr) # momentum=0.9\n",
    "    labels = torch.zeros(batchsize, 1)\n",
    "    \n",
    "    for i in range(iterations):  \n",
    "\n",
    "            inputs = torch.FloatTensor(batchsize, 1).uniform_(u, v)\n",
    "            inputs.requires_grad = True\n",
    "            \n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # gradient of output w.r.t. input\n",
    "            if regularize:\n",
    "                    output = net(inputs) \n",
    "                    gradient, = torch.autograd.grad(output, inputs, grad_outputs=output.data.new(output.shape).fill_(1),\n",
    "                           create_graph=True)\n",
    "                    max_grad = torch.max(abs(gradient))\n",
    "                    loss = criterion(net(inputs), labels) + reg_scale * max_grad\n",
    "            \n",
    "            else:\n",
    "                    loss = criterion(net(inputs), labels)\n",
    "            \n",
    "            \n",
    "            # forward + backward + optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            #tensorboard and print statistics\n",
    "            output = net(x_val)\n",
    "            error = torch.max(abs(output - y_val))\n",
    "            gradient, = torch.autograd.grad(output, x_val, grad_outputs=output.data.new(output.shape).fill_(1),\n",
    "                           create_graph=True)\n",
    "            max_grad = torch.max(abs(gradient))\n",
    "            \n",
    "            if regularize:\n",
    "                    valid_loss = criterion(output, y_val) + reg_scale * max_grad            \n",
    "            \n",
    "            else:\n",
    "                    valid_loss = criterion(output, y_val)\n",
    "                    \n",
    "            if logging:\n",
    "                    for name, param_tensor in net.named_parameters():\n",
    "                            writer.add_histogram(name, param_tensor.data.numpy(), i)\n",
    "                            for num, parameter in enumerate(param_tensor.data.flatten()):\n",
    "                                    writer.add_scalar('parameter/'+name+str(num), parameter, i)            \n",
    "                    writer.add_scalars('metrics', {'valid_loss': valid_loss, 'max. error': error, 'max. grad': max_grad,\n",
    "                                                         'batch_loss': loss}, i)\n",
    "\n",
    "            if i % 500 == 499:    # print every 500 mini-batches\n",
    "                    print('iteration: %5d   batch_loss: %.4f   valid_loss: %.4f   max error: %.6f   max grad: %.4f' %(i + 1, loss, valid_loss, error, max_grad))    \n",
    "    \n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "ih08I9JD2B6e",
    "outputId": "a0eef9eb-1833-4992-a070-dababf3f16ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:   500   batch_loss: 0.0000   valid_loss: 0.0000   max error: 0.005430   max grad: 0.0000\n",
      "iteration:  1000   batch_loss: 0.0000   valid_loss: 0.0000   max error: 0.002021   max grad: 0.0000\n",
      "iteration:  1500   batch_loss: 0.0000   valid_loss: 0.0000   max error: 0.000789   max grad: 0.0000\n",
      "iteration:  2000   batch_loss: 0.0000   valid_loss: 0.0000   max error: 0.000333   max grad: 0.0000\n",
      "iteration:  2500   batch_loss: 0.0000   valid_loss: 0.0000   max error: 0.000098   max grad: 0.0000\n",
      "iteration:  3000   batch_loss: 0.0000   valid_loss: 0.0000   max error: 0.000036   max grad: 0.0000\n",
      "iteration:  3500   batch_loss: 0.0000   valid_loss: 0.0000   max error: 0.000013   max grad: 0.0000\n",
      "iteration:  4000   batch_loss: 0.0000   valid_loss: 0.0000   max error: 0.000013   max grad: 0.0000\n",
      "iteration:  4500   batch_loss: 0.0000   valid_loss: 0.0000   max error: 0.000009   max grad: 0.0000\n",
      "iteration:  5000   batch_loss: 0.0000   valid_loss: 0.0000   max error: 0.000007   max grad: 0.0000\n",
      "iteration:  5500   batch_loss: 0.0000   valid_loss: 0.0000   max error: 0.000004   max grad: 0.0000\n",
      "iteration:  6000   batch_loss: 0.0000   valid_loss: 0.0000   max error: 0.000006   max grad: 0.0000\n",
      "iteration:  6500   batch_loss: 0.0000   valid_loss: 0.0000   max error: 0.000003   max grad: 0.0000\n",
      "iteration:  7000   batch_loss: 0.0000   valid_loss: 0.0000   max error: 0.000001   max grad: 0.0000\n",
      "iteration:  7500   batch_loss: 0.0000   valid_loss: 0.0000   max error: 0.000001   max grad: 0.0000\n",
      "iteration:  8000   batch_loss: 0.0000   valid_loss: 0.0000   max error: 0.000002   max grad: 0.0000\n",
      "iteration:  8500   batch_loss: 0.0000   valid_loss: 0.0000   max error: 0.000001   max grad: 0.0000\n",
      "iteration:  9000   batch_loss: 0.0000   valid_loss: 0.0000   max error: 0.000000   max grad: 0.0000\n",
      "iteration:  9500   batch_loss: 0.0000   valid_loss: 0.0000   max error: 0.000000   max grad: 0.0000\n",
      "iteration: 10000   batch_loss: 0.0000   valid_loss: 0.0000   max error: 0.000000   max grad: 0.0000\n",
      "iteration: 10500   batch_loss: 0.0000   valid_loss: 0.0000   max error: 0.000000   max grad: 0.0000\n",
      "iteration: 11000   batch_loss: 0.0000   valid_loss: 0.0000   max error: 0.000001   max grad: 0.0000\n",
      "iteration: 11500   batch_loss: 0.0000   valid_loss: 0.0000   max error: 0.000000   max grad: 0.0000\n",
      "iteration: 12000   batch_loss: 0.0000   valid_loss: 0.0000   max error: 0.000000   max grad: 0.0000\n",
      "iteration: 12500   batch_loss: 0.0000   valid_loss: 0.0000   max error: 0.000000   max grad: 0.0000\n",
      "iteration: 13000   batch_loss: 0.0000   valid_loss: 0.0000   max error: 0.000000   max grad: 0.0000\n",
      "iteration: 13500   batch_loss: 0.0000   valid_loss: 0.0000   max error: 0.000000   max grad: 0.0000\n",
      "iteration: 14000   batch_loss: 0.0000   valid_loss: 0.0000   max error: 0.000000   max grad: 0.0000\n",
      "iteration: 14500   batch_loss: 0.0000   valid_loss: 0.0000   max error: 0.000000   max grad: 0.0000\n",
      "iteration: 15000   batch_loss: 0.0000   valid_loss: 0.0000   max error: 0.000000   max grad: 0.0000\n",
      "iteration: 15500   batch_loss: 0.0000   valid_loss: 0.0000   max error: 0.000000   max grad: 0.0000\n",
      "iteration: 16000   batch_loss: 0.0000   valid_loss: 0.0000   max error: 0.000000   max grad: 0.0000\n",
      "iteration: 16500   batch_loss: 0.0000   valid_loss: 0.0000   max error: 0.000000   max grad: 0.0000\n",
      "iteration: 17000   batch_loss: 0.0000   valid_loss: 0.0000   max error: 0.000000   max grad: 0.0000\n",
      "iteration: 17500   batch_loss: 0.0000   valid_loss: 0.0000   max error: 0.000000   max grad: 0.0000\n",
      "iteration: 18000   batch_loss: 0.0000   valid_loss: 0.0000   max error: 0.000000   max grad: 0.0000\n",
      "iteration: 18500   batch_loss: 0.0000   valid_loss: 0.0000   max error: 0.000000   max grad: 0.0000\n",
      "iteration: 19000   batch_loss: 0.0000   valid_loss: 0.0000   max error: 0.000000   max grad: 0.0000\n",
      "iteration: 19500   batch_loss: 0.0000   valid_loss: 0.0000   max error: 0.000000   max grad: 0.0000\n",
      "iteration: 20000   batch_loss: 0.0000   valid_loss: 0.0000   max error: 0.000000   max grad: 0.0000\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "#network\n",
    "net=Net()\n",
    "parameter=optimize(iterations, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "id": "3YMzTw3cl6or",
    "outputId": "84de0421-ed17-47fe-c1af-eae25b080890"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plotting the predicted function\n",
    "\n",
    "delta = 3. #increase interval by delta\n",
    "\n",
    "x_plt = torch.linspace(u-delta, v+delta, m).view(-1,1)\n",
    "with torch.no_grad():\n",
    "    y_plt = net(x_plt)\n",
    "\n",
    "plt.plot(x_plt.numpy(), y_plt.numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.0052],\n",
      "        [-0.0208]], requires_grad=True)\n",
      "fc1.bias\n",
      "Parameter containing:\n",
      "tensor([-0.0261, -1.0424], requires_grad=True)\n",
      "fc2.weight\n",
      "Parameter containing:\n",
      "tensor([[4.3569e-05, 9.1583e-02]], requires_grad=True)\n",
      "fc2.bias\n",
      "Parameter containing:\n",
      "tensor([5.2854e-08], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for name, param_tensor in net.named_parameters():\n",
    "        print(name)\n",
    "        print(param_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jS2YELWL2aDY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorBoard 1.13.1 at http://vong:6006 (Press CTRL+C to quit)\n",
      "I0503 14:55:19.542445 140568330999552 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:55:19] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:55:19.608771 140568330999552 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:55:19] \"\u001b[37mGET /tf-interactive-inference-dashboard/editedexample.png HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:55:19.612882 140568372946688 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:55:19] \"\u001b[37mGET /tf-interactive-inference-dashboard/distance.png HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:55:19.614855 140568482002688 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:55:19] \"\u001b[37mGET /tf-interactive-inference-dashboard/explorecounterfactuals.png HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:55:19.618401 140567753254656 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:55:19] \"\u001b[37mGET /tf-interactive-inference-dashboard/pdplots.png HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:55:20.431128 140568482002688 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:55:20] \"\u001b[37mGET /data/environment HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:55:20.431374 140568330999552 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:55:20] \"\u001b[37mGET /data/experiments HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:55:20.481662 140567753254656 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:55:20] \"\u001b[37mGET /data/runs HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:55:20.499316 140568330999552 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:55:20] \"\u001b[37mGET /data/environment HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:55:20.504346 140567484819200 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:55:20] \"\u001b[37mGET /data/experiments HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:55:20.512541 140567753254656 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:55:20] \"\u001b[37mGET /data/runs HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:55:20.626669 140568372946688 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:55:20] \"\u001b[37mGET /data/plugins_listing HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:55:20.628897 140568482002688 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:55:20] \"\u001b[37mGET /data/plugins_listing HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:55:20.730451 140568372946688 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:55:20] \"\u001b[37mGET /data/plugin/scalars/tags HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:55:20.825146 140568372946688 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:55:20] \"\u001b[37mGET /data/plugin/scalars/tags HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:55:21.158146 140568372946688 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:55:21] \"\u001b[37mGET /data/plugin/scalars/scalars?tag=metrics&run=May03_14-53-45_vong%2Fmetrics%2Fbatch_loss&experiment= HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:55:21.158707 140568482002688 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:55:21] \"\u001b[37mGET /data/plugin/scalars/scalars?tag=parameter%2Ffc1.bias0&run=May03_14-53-45_vong&experiment= HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:55:21.175924 140567484819200 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:55:21] \"\u001b[37mGET /data/plugin/scalars/scalars?tag=parameter%2Ffc1.weight0&run=May03_14-53-45_vong&experiment= HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:55:21.182023 140568330999552 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:55:21] \"\u001b[37mGET /data/plugin/scalars/scalars?tag=parameter%2Ffc1.weight1&run=May03_14-53-45_vong&experiment= HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:55:21.193677 140567753254656 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:55:21] \"\u001b[37mGET /data/plugin/scalars/scalars?tag=parameter%2Ffc1.bias1&run=May03_14-53-45_vong&experiment= HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:55:21.236560 140568482002688 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:55:21] \"\u001b[37mGET /data/plugin/scalars/scalars?tag=parameter%2Ffc2.weight0&run=May03_14-53-45_vong&experiment= HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:55:21.237183 140567476426496 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:55:21] \"\u001b[37mGET /data/plugin/scalars/scalars?tag=parameter%2Ffc2.bias0&run=May03_14-53-45_vong&experiment= HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:55:21.290086 140568372946688 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:55:21] \"\u001b[37mGET /data/plugin/scalars/scalars?tag=parameter%2Ffc2.weight1&run=May03_14-53-45_vong&experiment= HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:55:21.403148 140567484819200 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:55:21] \"\u001b[37mGET /data/plugin/scalars/scalars?tag=parameter%2Ffc1.bias1&run=May03_14-53-45_vong&experiment= HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:55:21.406179 140567476426496 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:55:21] \"\u001b[37mGET /data/plugin/scalars/scalars?tag=parameter%2Ffc2.bias0&run=May03_14-53-45_vong&experiment= HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:55:21.406517 140567753254656 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:55:21] \"\u001b[37mGET /data/plugin/scalars/scalars?tag=parameter%2Ffc1.weight0&run=May03_14-53-45_vong&experiment= HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:55:21.426166 140568482002688 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:55:21] \"\u001b[37mGET /data/plugin/scalars/scalars?tag=parameter%2Ffc2.weight1&run=May03_14-53-45_vong&experiment= HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:55:21.428107 140568330999552 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:55:21] \"\u001b[37mGET /data/plugin/scalars/scalars?tag=parameter%2Ffc1.weight1&run=May03_14-53-45_vong&experiment= HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:55:21.430907 140568372946688 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:55:21] \"\u001b[37mGET /data/plugin/scalars/scalars?tag=metrics&run=May03_14-53-45_vong%2Fmetrics%2Fbatch_loss&experiment= HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:55:21.480252 140567484819200 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:55:21] \"\u001b[37mGET /data/plugin/scalars/scalars?tag=parameter%2Ffc2.weight0&run=May03_14-53-45_vong&experiment= HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:55:50.494809 140568482002688 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:55:50] \"\u001b[37mGET /data/runs HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:55:50.495070 140567476426496 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:55:50] \"\u001b[37mGET /data/experiments HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:55:50.497141 140567753254656 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:55:50] \"\u001b[37mGET /data/environment HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:55:50.498006 140568372946688 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:55:50] \"\u001b[37mGET /data/plugins_listing HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:55:50.516641 140568372946688 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:55:50] \"\u001b[37mGET /data/plugin/scalars/tags HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:55:50.708859 140567476426496 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:55:50] \"\u001b[37mGET /data/plugin/scalars/scalars?tag=parameter%2Ffc1.bias1&run=May03_14-53-45_vong&experiment= HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:55:50.714771 140567753254656 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:55:50] \"\u001b[37mGET /data/plugin/scalars/scalars?tag=parameter%2Ffc1.bias0&run=May03_14-53-45_vong&experiment= HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:55:50.715071 140568330999552 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:55:50] \"\u001b[37mGET /data/plugin/scalars/scalars?tag=parameter%2Ffc1.weight1&run=May03_14-53-45_vong&experiment= HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:55:50.715360 140568482002688 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:55:50] \"\u001b[37mGET /data/plugin/scalars/scalars?tag=metrics&run=May03_14-53-45_vong%2Fmetrics%2Fvalid_loss&experiment= HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:55:50.715801 140568372946688 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:55:50] \"\u001b[37mGET /data/plugin/scalars/scalars?tag=metrics&run=May03_14-53-45_vong%2Fmetrics%2Fmax.+error&experiment= HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:55:50.731717 140567484819200 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:55:50] \"\u001b[37mGET /data/plugin/scalars/scalars?tag=parameter%2Ffc1.weight0&run=May03_14-53-45_vong&experiment= HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:55:50.791624 140568482002688 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:55:50] \"\u001b[37mGET /data/plugin/scalars/scalars?tag=parameter%2Ffc2.weight0&run=May03_14-53-45_vong&experiment= HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:55:50.834586 140568330999552 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:55:50] \"\u001b[37mGET /data/plugin/scalars/scalars?tag=parameter%2Ffc2.weight1&run=May03_14-53-45_vong&experiment= HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:55:50.858618 140567753254656 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:55:50] \"\u001b[37mGET /data/plugin/scalars/scalars?tag=parameter%2Ffc2.bias0&run=May03_14-53-45_vong&experiment= HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:55:50.859166 140568372946688 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:55:50] \"\u001b[37mGET /data/plugin/scalars/scalars?tag=metrics&run=May03_14-53-45_vong%2Fmetrics%2Fvalid_loss&experiment= HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:55:50.862545 140567476426496 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:55:50] \"\u001b[37mGET /data/plugin/scalars/scalars?tag=metrics&run=May03_14-53-45_vong%2Fmetrics%2Fmax.+error&experiment= HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:55:50.863207 140567484819200 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:55:50] \"\u001b[37mGET /data/plugin/scalars/scalars?tag=parameter%2Ffc1.bias0&run=May03_14-53-45_vong&experiment= HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:55:50.863604 140568482002688 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:55:50] \"\u001b[37mGET /data/plugin/scalars/scalars?tag=parameter%2Ffc1.bias1&run=May03_14-53-45_vong&experiment= HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I0503 14:55:50.900195 140568330999552 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:55:50] \"\u001b[37mGET /data/plugin/scalars/scalars?tag=parameter%2Ffc1.weight0&run=May03_14-53-45_vong&experiment= HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:55:50.980213 140568372946688 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:55:50] \"\u001b[37mGET /data/plugin/scalars/scalars?tag=parameter%2Ffc1.weight1&run=May03_14-53-45_vong&experiment= HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:55:50.980978 140567476426496 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:55:50] \"\u001b[37mGET /data/plugin/scalars/scalars?tag=parameter%2Ffc2.bias0&run=May03_14-53-45_vong&experiment= HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:55:50.981367 140568482002688 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:55:50] \"\u001b[37mGET /data/plugin/scalars/scalars?tag=parameter%2Ffc2.weight1&run=May03_14-53-45_vong&experiment= HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:55:50.981991 140567753254656 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:55:50] \"\u001b[37mGET /data/plugin/scalars/scalars?tag=parameter%2Ffc2.weight0&run=May03_14-53-45_vong&experiment= HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:56:09.220570 140568330999552 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:56:09] \"\u001b[37mGET /data/plugin/scalars/scalars?tag=metrics&run=May03_14-53-45_vong%2Fmetrics%2Fmax.+grad&experiment= HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:56:18.119057 140568330999552 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:56:18] \"\u001b[37mGET /data/plugin/scalars/scalars?tag=metrics&run=May03_14-53-45_vong%2Fmetrics%2Fbatch_loss&experiment= HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:56:20.500454 140567753254656 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:56:20] \"\u001b[37mGET /data/runs HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:56:20.501623 140567476426496 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:56:20] \"\u001b[37mGET /data/experiments HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:56:20.502130 140568482002688 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:56:20] \"\u001b[37mGET /data/environment HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:56:20.503563 140568372946688 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:56:20] \"\u001b[37mGET /data/plugins_listing HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:56:20.523004 140568372946688 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:56:20] \"\u001b[37mGET /data/plugin/scalars/tags HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:56:20.681266 140568330999552 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:56:20] \"\u001b[37mGET /data/plugin/scalars/scalars?tag=parameter%2Ffc1.bias1&run=May03_14-53-45_vong&experiment= HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:56:20.706051 140568372946688 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:56:20] \"\u001b[37mGET /data/plugin/scalars/scalars?tag=metrics&run=May03_14-53-45_vong%2Fmetrics%2Fmax.+error&experiment= HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:56:20.706397 140567753254656 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:56:20] \"\u001b[37mGET /data/plugin/scalars/scalars?tag=metrics&run=May03_14-53-45_vong%2Fmetrics%2Fbatch_loss&experiment= HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:56:20.706949 140568482002688 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:56:20] \"\u001b[37mGET /data/plugin/scalars/scalars?tag=metrics&run=May03_14-53-45_vong%2Fmetrics%2Fmax.+grad&experiment= HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:56:20.712558 140567484819200 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:56:20] \"\u001b[37mGET /data/plugin/scalars/scalars?tag=parameter%2Ffc1.bias0&run=May03_14-53-45_vong&experiment= HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:56:20.712915 140567476426496 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:56:20] \"\u001b[37mGET /data/plugin/scalars/scalars?tag=metrics&run=May03_14-53-45_vong%2Fmetrics%2Fvalid_loss&experiment= HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:56:20.791939 140568330999552 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:56:20] \"\u001b[37mGET /data/plugin/scalars/scalars?tag=parameter%2Ffc1.weight0&run=May03_14-53-45_vong&experiment= HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:56:20.812946 140567753254656 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:56:20] \"\u001b[37mGET /data/plugin/scalars/scalars?tag=parameter%2Ffc1.weight1&run=May03_14-53-45_vong&experiment= HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:56:20.823468 140567484819200 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:56:20] \"\u001b[37mGET /data/plugin/scalars/scalars?tag=parameter%2Ffc2.bias0&run=May03_14-53-45_vong&experiment= HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:56:20.823974 140567476426496 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:56:20] \"\u001b[37mGET /data/plugin/scalars/scalars?tag=parameter%2Ffc2.weight1&run=May03_14-53-45_vong&experiment= HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:56:20.824272 140568372946688 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:56:20] \"\u001b[37mGET /data/plugin/scalars/scalars?tag=parameter%2Ffc2.weight0&run=May03_14-53-45_vong&experiment= HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:56:20.824509 140568482002688 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:56:20] \"\u001b[37mGET /data/plugin/scalars/scalars?tag=metrics&run=May03_14-53-45_vong%2Fmetrics%2Fmax.+error&experiment= HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:56:20.851539 140568330999552 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:56:20] \"\u001b[37mGET /data/plugin/scalars/scalars?tag=metrics&run=May03_14-53-45_vong%2Fmetrics%2Fmax.+grad&experiment= HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:56:20.952054 140567753254656 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:56:20] \"\u001b[37mGET /data/plugin/scalars/scalars?tag=parameter%2Ffc1.weight0&run=May03_14-53-45_vong&experiment= HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:56:20.965167 140567484819200 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:56:20] \"\u001b[37mGET /data/plugin/scalars/scalars?tag=parameter%2Ffc2.weight0&run=May03_14-53-45_vong&experiment= HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:56:20.965590 140567476426496 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:56:20] \"\u001b[37mGET /data/plugin/scalars/scalars?tag=parameter%2Ffc1.weight1&run=May03_14-53-45_vong&experiment= HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:56:20.965909 140568372946688 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:56:20] \"\u001b[37mGET /data/plugin/scalars/scalars?tag=parameter%2Ffc2.weight1&run=May03_14-53-45_vong&experiment= HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:56:20.966461 140568482002688 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:56:20] \"\u001b[37mGET /data/plugin/scalars/scalars?tag=parameter%2Ffc2.bias0&run=May03_14-53-45_vong&experiment= HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:56:21.399477 140568330999552 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:56:21] \"\u001b[37mGET /data/plugin/scalars/scalars?tag=metrics&run=May03_14-53-45_vong%2Fmetrics%2Fvalid_loss&experiment= HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:56:27.433296 140568330999552 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:56:27] \"\u001b[37mGET /data/plugin/histograms/tags HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:56:27.680310 140568372946688 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:56:27] \"\u001b[37mGET /data/plugin/histograms/histograms?run=May03_14-53-45_vong&tag=fc1.bias HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:56:27.681020 140568482002688 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:56:27] \"\u001b[37mGET /data/plugin/histograms/histograms?run=May03_14-53-45_vong&tag=fc1.weight HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:56:29.768672 140568330999552 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:56:29] \"\u001b[37mGET /data/plugin/distributions/tags HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:56:30.203276 140568482002688 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:56:30] \"\u001b[37mGET /data/plugin/distributions/distributions?run=May03_14-53-45_vong&tag=fc1.weight HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:56:30.204092 140568372946688 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:56:30] \"\u001b[37mGET /data/plugin/distributions/distributions?run=May03_14-53-45_vong&tag=fc1.bias HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:56:32.334301 140568330999552 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:56:32] \"\u001b[37mGET /data/plugin/distributions/distributions?run=May03_14-53-45_vong&tag=fc2.bias HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:56:35.555337 140568330999552 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:56:35] \"\u001b[37mGET /data/plugin/distributions/distributions?run=May03_14-53-45_vong&tag=fc2.weight HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:56:46.977725 140568372946688 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:56:46] \"\u001b[37mGET /data/plugin/histograms/histograms?run=May03_14-53-45_vong&tag=fc2.bias HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I0503 14:56:49.763097 140568372946688 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:56:49] \"\u001b[37mGET /data/plugin/histograms/histograms?run=May03_14-53-45_vong&tag=fc2.weight HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:56:50.503228 140567753254656 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:56:50] \"\u001b[37mGET /data/runs HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:56:50.503588 140568482002688 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:56:50] \"\u001b[37mGET /data/environment HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:56:50.504485 140567484819200 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:56:50] \"\u001b[37mGET /data/experiments HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:56:50.504983 140568372946688 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:56:50] \"\u001b[37mGET /data/plugins_listing HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:56:50.517969 140568372946688 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:56:50] \"\u001b[37mGET /data/plugin/histograms/tags HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:56:50.605828 140567476426496 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:56:50] \"\u001b[37mGET /data/plugin/histograms/histograms?run=May03_14-53-45_vong&tag=fc2.bias HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:56:50.611559 140567753254656 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:56:50] \"\u001b[37mGET /data/plugin/histograms/histograms?run=May03_14-53-45_vong&tag=fc1.weight HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:56:50.637961 140568372946688 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:56:50] \"\u001b[37mGET /data/plugin/histograms/histograms?run=May03_14-53-45_vong&tag=fc1.bias HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:56:50.725771 140568482002688 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:56:50] \"\u001b[37mGET /data/plugin/histograms/histograms?run=May03_14-53-45_vong&tag=fc2.weight HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:57:20.526674 140568482002688 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:57:20] \"\u001b[37mGET /data/environment HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:57:20.527026 140567484819200 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:57:20] \"\u001b[37mGET /data/experiments HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:57:20.530192 140567753254656 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:57:20] \"\u001b[37mGET /data/runs HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:57:20.531033 140568372946688 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:57:20] \"\u001b[37mGET /data/plugins_listing HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:57:20.542119 140568482002688 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:57:20] \"\u001b[37mGET /data/plugin/histograms/tags HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:57:20.674844 140568372946688 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:57:20] \"\u001b[37mGET /data/plugin/histograms/histograms?run=May03_14-53-45_vong&tag=fc1.bias HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:57:20.690623 140567753254656 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:57:20] \"\u001b[37mGET /data/plugin/histograms/histograms?run=May03_14-53-45_vong&tag=fc2.bias HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:57:20.721244 140568482002688 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:57:20] \"\u001b[37mGET /data/plugin/histograms/histograms?run=May03_14-53-45_vong&tag=fc1.weight HTTP/1.1\u001b[0m\" 200 -\n",
      "I0503 14:57:20.810387 140567476426496 _internal.py:122] ::ffff:127.0.0.1 - - [03/May/2019 14:57:20] \"\u001b[37mGET /data/plugin/histograms/histograms?run=May03_14-53-45_vong&tag=fc2.weight HTTP/1.1\u001b[0m\" 200 -\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "if logging:\n",
    "        writer.close()\n",
    "        !tensorboard --logdir runs "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "invers_stability.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
